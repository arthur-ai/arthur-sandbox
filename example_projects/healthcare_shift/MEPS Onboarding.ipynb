{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arthurai import ArthurAI\n",
    "from arthurai.common.constants import InputType, OutputType, Stage, ValueType\n",
    "from arthurai.core.attributes import AttributeCategory, AttributeBin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up connection to API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"app.arthur.ai\"\n",
    "ACCESS_KEY = \"\"\n",
    "\n",
    "connection = ArthurAI(url=URL, access_key=ACCESS_KEY, client_version=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = connection.model(partner_model_id=\"MEPS_drift\",\n",
    "                         input_type=InputType.Tabular,\n",
    "                         output_type=OutputType.Multiclass,\n",
    "                         is_batch=True)\n",
    "\n",
    "# uncomment the below if you want to get the same model object that you have already created\n",
    "# model = connection.get_model('MEPS_drift', id_type='partner_model_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up ArthurModel\n",
    "Everything under this header is *necessary* for an `ArthurModel` to be created; additional functionality is not \n",
    "possible until after `model.save()` has been successfully called.\n",
    "\n",
    "*Some context about this dataset:*\n",
    "\n",
    "- label: `UTILIZATION`, where 1: >10 visits, 0: <10 visits\n",
    "\n",
    "- protected attribute: `RACE`, where 1: `White`, 0: `Non-White`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_data_full contains the X's and the Y's\n",
    "all_data = pd.read_parquet('data/fulldata_train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>RACE</th>\n",
       "      <th>PCS42</th>\n",
       "      <th>MCS42</th>\n",
       "      <th>K6SUM42</th>\n",
       "      <th>REGION1</th>\n",
       "      <th>REGION2</th>\n",
       "      <th>REGION3</th>\n",
       "      <th>REGION4</th>\n",
       "      <th>SEX1</th>\n",
       "      <th>...</th>\n",
       "      <th>POVCAT2</th>\n",
       "      <th>POVCAT3</th>\n",
       "      <th>POVCAT4</th>\n",
       "      <th>POVCAT5</th>\n",
       "      <th>INSCOV1</th>\n",
       "      <th>INSCOV2</th>\n",
       "      <th>INSCOV3</th>\n",
       "      <th>p_0</th>\n",
       "      <th>p_1</th>\n",
       "      <th>gt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.70</td>\n",
       "      <td>57.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.908582</td>\n",
       "      <td>0.091418</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.48</td>\n",
       "      <td>57.54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.954603</td>\n",
       "      <td>0.045397</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.886550</td>\n",
       "      <td>0.113450</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.922157</td>\n",
       "      <td>0.077843</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>42.10</td>\n",
       "      <td>61.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.978833</td>\n",
       "      <td>0.021167</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    AGE  RACE  PCS42  MCS42  K6SUM42  REGION1  REGION2  REGION3  REGION4  \\\n",
       "0  54.0   0.0  34.70  57.25      0.0      0.0      1.0      0.0      0.0   \n",
       "1  25.0   0.0  50.48  57.54      0.0      0.0      1.0      0.0      0.0   \n",
       "2  37.0   0.0  -1.00  -1.00     -1.0      0.0      0.0      0.0      1.0   \n",
       "3  29.0   0.0  -1.00  -1.00     -1.0      1.0      0.0      0.0      0.0   \n",
       "4  84.0   1.0  42.10  61.02      0.0      1.0      0.0      0.0      0.0   \n",
       "\n",
       "   SEX1  ...  POVCAT2  POVCAT3  POVCAT4  POVCAT5  INSCOV1  INSCOV2  INSCOV3  \\\n",
       "0   0.0  ...      0.0      1.0      0.0      0.0      1.0      0.0      0.0   \n",
       "1   0.0  ...      0.0      1.0      0.0      0.0      0.0      1.0      0.0   \n",
       "2   1.0  ...      0.0      0.0      1.0      0.0      0.0      0.0      1.0   \n",
       "3   1.0  ...      0.0      0.0      0.0      1.0      1.0      0.0      0.0   \n",
       "4   1.0  ...      0.0      0.0      0.0      1.0      0.0      1.0      0.0   \n",
       "\n",
       "        p_0       p_1   gt  \n",
       "0  0.908582  0.091418  0.0  \n",
       "1  0.954603  0.045397  1.0  \n",
       "2  0.886550  0.113450  0.0  \n",
       "3  0.922157  0.077843  0.0  \n",
       "4  0.978833  0.021167  1.0  \n",
       "\n",
       "[5 rows x 141 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick summary of what all_data looks like\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "['p_0', 'p_1', 'gt']"
      ],
      "text/plain": [
       "['p_0', 'p_1', 'gt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.columns.tolist()[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the attributes used for training to the model and set them to ModelPipelineInput\n",
    "# note that attribute names need to contain only letters, numbers, and underscores, and cannot begin with a number\n",
    "train_x = all_data.drop(columns=['RACE','p_0', 'p_1', 'gt'])\n",
    "model.from_dataframe(train_x, Stage.ModelPipelineInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred_high_utilization': ArthurAttribute(name='pred_high_utilization', value_type='FLOAT', stage='PREDICTED_VALUE', id=None, label=None, position=0, categorical=False, min_range=0, max_range=1, monitor_for_bias=False, categories=None, bins=None, is_unique=False, is_positive_predicted_attribute=True, attribute_link='gt_high_utilization'),\n",
       " 'gt_high_utilization': ArthurAttribute(name='gt_high_utilization', value_type='INTEGER', stage='GROUND_TRUTH', id=None, label=None, position=0, categorical=True, min_range=None, max_range=None, monitor_for_bias=False, categories=[AttributeCategory(value='0', label=None), AttributeCategory(value='1', label=None)], bins=None, is_unique=False, is_positive_predicted_attribute=False, attribute_link='pred_high_utilization'),\n",
       " 'pred_low_utilization': ArthurAttribute(name='pred_low_utilization', value_type='FLOAT', stage='PREDICTED_VALUE', id=None, label=None, position=1, categorical=False, min_range=0, max_range=1, monitor_for_bias=False, categories=None, bins=None, is_unique=False, is_positive_predicted_attribute=False, attribute_link='gt_low_utilization'),\n",
       " 'gt_low_utilization': ArthurAttribute(name='gt_low_utilization', value_type='INTEGER', stage='GROUND_TRUTH', id=None, label=None, position=1, categorical=True, min_range=None, max_range=None, monitor_for_bias=False, categories=[AttributeCategory(value='0', label=None), AttributeCategory(value='1', label=None)], bins=None, is_unique=False, is_positive_predicted_attribute=False, attribute_link='pred_low_utilization')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gt_<>_utilization will refer to the ground truth values (i.e. true labels), while\n",
    "# pred_<>_utilization will refer to the predicted outputs of your model. \n",
    "pred_to_ground_truth_map = {\n",
    "    \"pred_high_utilization\": \"gt_high_utilization\",\n",
    "    \"pred_low_utilization\": \"gt_low_utilization\"\n",
    "}\n",
    "\n",
    "# add the ground truth and predicted attributes to the model\n",
    "model.add_binary_classifier_output_attributes('pred_high_utilization', pred_to_ground_truth_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "[ArthurAttribute(name='RACE', value_type='INTEGER', stage='NON_INPUT_DATA', id=None, label=None, position=None, categorical=True, min_range=None, max_range=None, monitor_for_bias=True, categories=[AttributeCategory(value='1', label=None), AttributeCategory(value='0', label=None)], bins=None, is_unique=False, is_positive_predicted_attribute=False, attribute_link=None)]"
      ],
      "text/plain": [
       "[ArthurAttribute(name='RACE', value_type='INTEGER', stage='NON_INPUT_DATA', id=None, label=None, position=None, categorical=True, min_range=None, max_range=None, monitor_for_bias=True, categories=[AttributeCategory(value='1', label=None), AttributeCategory(value='0', label=None)], bins=None, is_unique=False, is_positive_predicted_attribute=False, attribute_link=None)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add bias monitoring - this cannot be done after the model is saved\n",
    "# to monitor for bias, 'RACE' must be an int (or take on discrete string values)\n",
    "\n",
    "model.add_attribute(name='RACE', \n",
    "     stage=Stage.NonInputData,\n",
    "     value_type=ValueType.Integer, \n",
    "     categorical=True, \n",
    "     categories=[AttributeCategory(value=1), AttributeCategory(value=0)],\n",
    "     monitor_for_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>stage</th>\n",
       "      <th>value_type</th>\n",
       "      <th>categorical</th>\n",
       "      <th>is_unique</th>\n",
       "      <th>categories</th>\n",
       "      <th>bins</th>\n",
       "      <th>range</th>\n",
       "      <th>monitor_for_bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGE</td>\n",
       "      <td>PIPELINE_INPUT</td>\n",
       "      <td>FLOAT</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.0, 85.0]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PCS42</td>\n",
       "      <td>PIPELINE_INPUT</td>\n",
       "      <td>FLOAT</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[-9.0, 62.56]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MCS42</td>\n",
       "      <td>PIPELINE_INPUT</td>\n",
       "      <td>FLOAT</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[-9.0, 65.95]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K6SUM42</td>\n",
       "      <td>PIPELINE_INPUT</td>\n",
       "      <td>FLOAT</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[-9.0, 15.0]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>REGION1</td>\n",
       "      <td>PIPELINE_INPUT</td>\n",
       "      <td>FLOAT</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[{value: 0.0}, {value: 1.0}]</td>\n",
       "      <td>None</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>gt_high_utilization</td>\n",
       "      <td>GROUND_TRUTH</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[{value: 0}, {value: 1}]</td>\n",
       "      <td>None</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>pred_high_utilization</td>\n",
       "      <td>PREDICTED_VALUE</td>\n",
       "      <td>FLOAT</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>gt_low_utilization</td>\n",
       "      <td>GROUND_TRUTH</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[{value: 0}, {value: 1}]</td>\n",
       "      <td>None</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>pred_low_utilization</td>\n",
       "      <td>PREDICTED_VALUE</td>\n",
       "      <td>FLOAT</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>RACE</td>\n",
       "      <td>NON_INPUT_DATA</td>\n",
       "      <td>INTEGER</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[{value: 1}, {value: 0}]</td>\n",
       "      <td>None</td>\n",
       "      <td>[None, None]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name            stage value_type categorical is_unique  \\\n",
       "0                      AGE   PIPELINE_INPUT      FLOAT       False     False   \n",
       "1                    PCS42   PIPELINE_INPUT      FLOAT       False     False   \n",
       "2                    MCS42   PIPELINE_INPUT      FLOAT       False     False   \n",
       "3                  K6SUM42   PIPELINE_INPUT      FLOAT       False     False   \n",
       "4                  REGION1   PIPELINE_INPUT      FLOAT        True     False   \n",
       "..                     ...              ...        ...         ...       ...   \n",
       "137    gt_high_utilization     GROUND_TRUTH    INTEGER        True     False   \n",
       "138  pred_high_utilization  PREDICTED_VALUE      FLOAT       False     False   \n",
       "139     gt_low_utilization     GROUND_TRUTH    INTEGER        True     False   \n",
       "140   pred_low_utilization  PREDICTED_VALUE      FLOAT       False     False   \n",
       "141                   RACE   NON_INPUT_DATA    INTEGER        True     False   \n",
       "\n",
       "                       categories  bins          range monitor_for_bias  \n",
       "0                              []  None    [0.0, 85.0]            False  \n",
       "1                              []  None  [-9.0, 62.56]            False  \n",
       "2                              []  None  [-9.0, 65.95]            False  \n",
       "3                              []  None   [-9.0, 15.0]            False  \n",
       "4    [{value: 0.0}, {value: 1.0}]  None   [None, None]            False  \n",
       "..                            ...   ...            ...              ...  \n",
       "137      [{value: 0}, {value: 1}]  None   [None, None]            False  \n",
       "138                            []  None         [0, 1]            False  \n",
       "139      [{value: 0}, {value: 1}]  None   [None, None]            False  \n",
       "140                            []  None         [0, 1]            False  \n",
       "141      [{value: 1}, {value: 0}]  None   [None, None]             True  \n",
       "\n",
       "[142 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check all the attributes loaded to the model. note that even though you passed in your \"real\" training\n",
    "# data, currently the model has no reference to the actual datapoints -- just the properties of the \n",
    "# attributes. you will need to set reference data (for data drift detection) later\n",
    "model.review()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2144927e-3906-4724-9a62-d9062ef06ee8'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your model is not uploaded to Arthur until you call model.save(). \n",
    "model.save()\n",
    "# model.update() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding additional functionality\n",
    "\n",
    "Setting reference data and monitoring for bias adds to the functionality of the `ArthurModel` that you've created. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting reference data\n",
    "\n",
    "Now that we've saved the model, we can set reference data.\n",
    "The reference data df must have a column for each `ModelPipelineInput` and each `NonInput`. Optionally, it \n",
    "can contain predicted value and ground truth columns; this will enable the calculation of data drift \n",
    "on output attributes. In this model, our ground truth columns are `gt_high_utilization` and `gt_low_utilization`,\n",
    "and our predicted value columns are `pred_high_utilization` and `pred_low_utilization`. \n",
    "\n",
    "We need to rename and update the columns in `all_data` to match these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['p_0', 'p_1', 'gt']\n",
    "all_data = all_data.rename(columns={'p_0': 'pred_low_utilization', 'p_1': 'pred_high_utilization'})\n",
    "gt = pd.get_dummies(all_data['gt'], prefix='gt', dtype='int')\n",
    "gt = gt.rename(columns={'gt_0.0': 'gt_low_utilization', 'gt_1.0': 'gt_high_utilization'})\n",
    "all_data = all_data.drop(columns=['gt'])\n",
    "all_data = all_data.merge(gt, left_index=True, right_index=True)\n",
    "all_data = all_data.astype({'RACE': 'int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AGE                      float64\n",
       "RACE                       int64\n",
       "PCS42                    float64\n",
       "MCS42                    float64\n",
       "K6SUM42                  float64\n",
       "REGION1                  float64\n",
       "REGION2                  float64\n",
       "REGION3                  float64\n",
       "REGION4                  float64\n",
       "SEX1                     float64\n",
       "SEX2                     float64\n",
       "MARRY1                   float64\n",
       "MARRY2                   float64\n",
       "MARRY3                   float64\n",
       "MARRY4                   float64\n",
       "MARRY5                   float64\n",
       "MARRY6                   float64\n",
       "MARRY7                   float64\n",
       "MARRY8                   float64\n",
       "MARRY9                   float64\n",
       "MARRY10                  float64\n",
       "FTSTU01                  float64\n",
       "FTSTU1                   float64\n",
       "FTSTU2                   float64\n",
       "FTSTU3                   float64\n",
       "ACTDTY1                  float64\n",
       "ACTDTY2                  float64\n",
       "ACTDTY3                  float64\n",
       "ACTDTY4                  float64\n",
       "HONRDC1                  float64\n",
       "HONRDC2                  float64\n",
       "HONRDC3                  float64\n",
       "HONRDC4                  float64\n",
       "RTHLTH01                 float64\n",
       "RTHLTH1                  float64\n",
       "RTHLTH2                  float64\n",
       "RTHLTH3                  float64\n",
       "RTHLTH4                  float64\n",
       "RTHLTH5                  float64\n",
       "MNHLTH01                 float64\n",
       "MNHLTH1                  float64\n",
       "MNHLTH2                  float64\n",
       "MNHLTH3                  float64\n",
       "MNHLTH4                  float64\n",
       "MNHLTH5                  float64\n",
       "HIBPDX01                 float64\n",
       "HIBPDX1                  float64\n",
       "HIBPDX2                  float64\n",
       "CHDDX01                  float64\n",
       "CHDDX1                   float64\n",
       "CHDDX2                   float64\n",
       "ANGIDX01                 float64\n",
       "ANGIDX1                  float64\n",
       "ANGIDX2                  float64\n",
       "MIDX01                   float64\n",
       "MIDX1                    float64\n",
       "MIDX2                    float64\n",
       "OHRTDX01                 float64\n",
       "OHRTDX1                  float64\n",
       "OHRTDX2                  float64\n",
       "STRKDX01                 float64\n",
       "STRKDX1                  float64\n",
       "STRKDX2                  float64\n",
       "EMPHDX01                 float64\n",
       "EMPHDX1                  float64\n",
       "EMPHDX2                  float64\n",
       "CHBRON01                 float64\n",
       "CHBRON1                  float64\n",
       "CHBRON2                  float64\n",
       "CHOLDX01                 float64\n",
       "CHOLDX1                  float64\n",
       "CHOLDX2                  float64\n",
       "CANCERDX01               float64\n",
       "CANCERDX1                float64\n",
       "CANCERDX2                float64\n",
       "DIABDX01                 float64\n",
       "DIABDX1                  float64\n",
       "DIABDX2                  float64\n",
       "JTPAIN01                 float64\n",
       "JTPAIN1                  float64\n",
       "JTPAIN2                  float64\n",
       "ARTHDX01                 float64\n",
       "ARTHDX1                  float64\n",
       "ARTHDX2                  float64\n",
       "ARTHTYPE01               float64\n",
       "ARTHTYPE1                float64\n",
       "ARTHTYPE2                float64\n",
       "ARTHTYPE3                float64\n",
       "ASTHDX1                  float64\n",
       "ASTHDX2                  float64\n",
       "ADHDADDX01               float64\n",
       "ADHDADDX1                float64\n",
       "ADHDADDX2                float64\n",
       "PREGNT01                 float64\n",
       "PREGNT1                  float64\n",
       "PREGNT2                  float64\n",
       "WLKLIM01                 float64\n",
       "WLKLIM1                  float64\n",
       "WLKLIM2                  float64\n",
       "ACTLIM01                 float64\n",
       "ACTLIM1                  float64\n",
       "ACTLIM2                  float64\n",
       "SOCLIM01                 float64\n",
       "SOCLIM1                  float64\n",
       "SOCLIM2                  float64\n",
       "COGLIM01                 float64\n",
       "COGLIM1                  float64\n",
       "COGLIM2                  float64\n",
       "DFHEAR4201               float64\n",
       "DFHEAR421                float64\n",
       "DFHEAR422                float64\n",
       "DFSEE4201                float64\n",
       "DFSEE421                 float64\n",
       "DFSEE422                 float64\n",
       "ADSMOK4201               float64\n",
       "ADSMOK421                float64\n",
       "ADSMOK422                float64\n",
       "PHQ24201                 float64\n",
       "PHQ2420                  float64\n",
       "PHQ2421                  float64\n",
       "PHQ2422                  float64\n",
       "PHQ2423                  float64\n",
       "PHQ2424                  float64\n",
       "PHQ2425                  float64\n",
       "PHQ2426                  float64\n",
       "EMPST01                  float64\n",
       "EMPST1                   float64\n",
       "EMPST2                   float64\n",
       "EMPST3                   float64\n",
       "EMPST4                   float64\n",
       "POVCAT1                  float64\n",
       "POVCAT2                  float64\n",
       "POVCAT3                  float64\n",
       "POVCAT4                  float64\n",
       "POVCAT5                  float64\n",
       "INSCOV1                  float64\n",
       "INSCOV2                  float64\n",
       "INSCOV3                  float64\n",
       "pred_low_utilization     float64\n",
       "pred_high_utilization    float64\n",
       "gt_low_utilization         int64\n",
       "gt_high_utilization        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 200)\n",
    "all_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'counts': {'success': 138, 'failure': 0, 'total': 138}, 'failures': [[]]}, {'dataset_close_result': 'success'})\n"
     ]
    }
   ],
   "source": [
    "res = model.set_reference_data(data=all_data) # note: need to explicitly specify the data= argument\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### send batches of inferences\n",
    "\n",
    "We do not need to load/expose the actual predictive model to send inferences to `arthur_model`. `x_shift` is a directory holding batched data. Each of the parquet files is a dataframe with the following columns:\n",
    "\n",
    "- sensitive attribute, `RACE`\n",
    "- all attributes used for training\n",
    "- `p_0`: the predicted probability of this example being in the 0 class - *given by your model*\n",
    "- `p_1`: the predicted probability of this example being in the 1 class - *given by your model*\n",
    "- `gt`: the ground truth label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fulldata_0.parquet  fulldata_2.parquet  fulldata_4.parquet  fulldata_6.parquet\r\n",
      "fulldata_1.parquet  fulldata_3.parquet  fulldata_5.parquet  fulldata_7.parquet\r\n"
     ]
    }
   ],
   "source": [
    "%ls data/x_shift "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### batch inference format\n",
    "(df option)\n",
    "Your batch inferences must have the dummied prediction columns that you created earlier; \n",
    "if you don't have ground truth labels at this time, you don't need to send them now.\n",
    "When sending batch inferences, you must add the following additional columns: \n",
    "- `batch_id` - string\n",
    "- `partner_inference_id`- string\n",
    "- `inference_timestamp` - datetime.datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'counts': {'success': 200, 'failure': 0, 'total': 200}, 'failures': [[]]}, {'dataset_close_result': 'success'})\n",
      "({'counts': {'success': 192, 'failure': 0, 'total': 192}, 'failures': [[]]}, {'dataset_close_result': 'success'})\n",
      "({'counts': {'success': 187, 'failure': 0, 'total': 187}, 'failures': [[]]}, {'dataset_close_result': 'success'})\n",
      "({'counts': {'success': 184, 'failure': 0, 'total': 184}, 'failures': [[]]}, {'dataset_close_result': 'success'})\n"
     ]
    }
   ],
   "source": [
    "for i in range(4): # sending inferences without ground truth\n",
    "    alldata = pd.read_parquet('data/x_shift/fulldata_'+str(i) +'.parquet')\n",
    "    tosend = alldata.rename(columns={'p_0': 'pred_low_utilization', 'p_1': 'pred_high_utilization'})\n",
    "    tosend['RACE'] = tosend['RACE'].astype(int)\n",
    "    \n",
    "    tosend['partner_inference_id'] = [str(np.random.randint(10000))]*len(tosend)\n",
    "    tosend['inference_timestamp'] = [(datetime.utcnow())]*len(tosend)\n",
    "    \n",
    "    res = model.send_batch_inferences(data=tosend, batch_id=str(i))\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'counts': {'success': 200, 'failure': 0, 'total': 200}, 'failures': [[]]}\n",
      "{'counts': {'success': 192, 'failure': 0, 'total': 192}, 'failures': [[]]}\n",
      "{'counts': {'success': 187, 'failure': 0, 'total': 187}, 'failures': [[]]}\n",
      "{'counts': {'success': 184, 'failure': 0, 'total': 184}, 'failures': [[]]}\n"
     ]
    }
   ],
   "source": [
    "for i in range(4): # adding ground truth later\n",
    "    alldata = pd.read_parquet('data/x_shift/fulldata_'+str(i) +'.parquet')\n",
    "    tosend = pd.get_dummies(alldata['gt'], prefix='gt')\n",
    "    tosend = tosend.rename(columns={'gt_0.0': 'gt_low_utilization', 'gt_1.0': 'gt_high_utilization'})\n",
    "    tosend = tosend.astype({'gt_low_utilization': 'int64', 'gt_high_utilization': 'int64'})\n",
    "\n",
    "    tosend['partner_inference_id'] = [str(np.random.randint(10000))]*len(tosend)\n",
    "    tosend['ground_truth_timestamp'] = [(datetime.utcnow())]*len(tosend)\n",
    "    \n",
    "    res = model.send_batch_ground_truths(data=tosend)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'counts': {'success': 195, 'failure': 0, 'total': 195}, 'failures': [[]]}, {'dataset_close_result': 'success'})\n",
      "({'counts': {'success': 214, 'failure': 0, 'total': 214}, 'failures': [[]]}, {'dataset_close_result': 'success'})\n",
      "({'counts': {'success': 209, 'failure': 0, 'total': 209}, 'failures': [[]]}, {'dataset_close_result': 'success'})\n",
      "({'counts': {'success': 219, 'failure': 0, 'total': 219}, 'failures': [[]]}, {'dataset_close_result': 'success'})\n"
     ]
    }
   ],
   "source": [
    "for i in range(4): # adding gt and infs at the same time\n",
    "    alldata = pd.read_parquet('data/x_shift/fulldata_'+str(i+4) +'.parquet')\n",
    "    tosend = alldata.rename(columns={'p_0': 'pred_low_utilization', 'p_1': 'pred_high_utilization'})\n",
    "    tosend['RACE'] = tosend['RACE'].astype(int)\n",
    "    \n",
    "    gts = pd.get_dummies(alldata['gt'], prefix='gt')\n",
    "    gts = gts.rename(columns={'gt_0.0': 'gt_low_utilization', 'gt_1.0': 'gt_high_utilization'})\n",
    "    \n",
    "    tosend = tosend.merge(gts, left_index=True, right_index=True)\n",
    "\n",
    "    tosend['partner_inference_id'] = [str(np.random.randint(10000))]*len(tosend)\n",
    "    tosend['inference_timestamp'] = [(datetime.utcnow())]*len(tosend)\n",
    "    \n",
    "    res = model.send_batch_inferences(data=tosend, batch_id=str(i+4))\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
